// Module included in the following assemblies
//
// * about/serverless-release-notes.adoc

:_content-type: REFERENCE
[id="serverless-rn-1-37-0_{context}"]
= Red Hat {ServerlessProductName} 1.37

{ServerlessProductName} 1.37 is now available. New features, updates, fixed issues, and known issues that pertain to {ServerlessProductName} on {ocp-product-title} are included in the following notes:

[id="new-features-1-37-0_{context}"]
== New features

[id="new-features-eventing-1-37-0_{context}"]
=== {ServerlessProductName} Eventing

* {ServerlessProductName} now uses Knative Eventing 1.17.

* {ServerlessProductName} now uses Knative for Apache Kafka 1.17.

* {ServerlessProductName} now supports deployment on ARM architecture as a Generally Available (GA) feature.

* Knative Eventing now supports the ability to define authorization policies that restrict which entities can send events to Eventing custom resources. This enables greater control and security within event-driven architectures. This functionality is now available as a Technology Preview.

[id="new-features-serving-1-37-0_{context}"]
=== {ServerlessProductName} Serving

* {ServerlessProductName} now uses Knative Serving 1.17.

* {ServerlessProductName} now uses Kourier 1.17.

* {ServerlessProductName} now uses Knative (`kn`) CLI 1.17.

[id="new-features-functions-1-37-0_{context}"]
=== {ServerlessProductName} Functions

* The `kn func` CLI plugin now uses `func` 1.17.

* Python runtime for {ServerlessProductName} Functions are now Generally Available (GA).

[id="new-features-osl-1-37-0_{context}"]
=== {ServerlessLogicProductName}

* {ServerlessLogicProductName} introduces a new data index mutation named `ExecuteAfter`, which enables you to create and execute a new workflow instance that can reuse the output of a previously completed workflow as its input.
+
The `ExecuteAfter` mutation accepts the following arguments:

* `processId`: The process ID of the workflow definition to be executed.
* `processVersion`: The process version of the workflow definition to be executed.
* `completedInstanceId` (optional): The ID of a previously completed workflow whose output is used as input for the new workflow instance.
* `input` (optional): Additional input data that is merged with the output of the `completedInstanceId`, if provided.
* `excludeProperties` (optional): A list of properties that are not copied from the `completedInstanceId` output into the new workflow instance input.

[id="fixed-issues-1-37-0_{context}"]
== Fixed issues

[id="fixed-issues-eventing-1-37-0_{context}"]
=== {ServerlessProductName} Eventing

* Before this update, the KafkaSource dispatcher stopped committing offsets when the offsets of produced events were not consecutive integers, for example, when events were produced within a Kafka transaction. This behavior caused the dispatcher to stall and prevented subsequent events from being processed.
+
With this update, the KafkaSource dispatcher has been fixed to handle such “empty” offsets correctly. Additionally, the default Kafka consumer configuration for KafkaSource has been updated to `isolation.level=read_committed`. When Kafka transactions are used to produce events into a Kafka topic, the KafkaSource now processes only the events from committed transactions.

[id="known-issues-1-37-0_{context}"]
== Known issues

[id="known-issues-eventing-1-37-0_{context}"]
=== {ServerlessProductName} Eventing

* The `EventTransform` custom resource definition (CRD) is currently not compatible with {SMProductName}. The EventT`ransform resource does not provide a way to configure Istio-specific labels or annotations required for integration with {SMProductName}. As a result, the `EventTransform` component cannot function properly in environments where {SMProductName} is enabled.

[id="known-issues-serving-1-37-0_{context}"]
=== {ServerlessProductName} Serving

* In some cases, cluster-scoped resources such as webhook configurations are not removed during the uninstallation, reinstallation, or upgrade of the `KnativeServing` or Serverless Operator components. When this occurs, the reconciliation of `KnativeServing` fails, and the installation process becomes stuck with an error similar to the following:
+
[source,text]
----
failed to apply non rbac manifest: Internal error occurred: failed calling webhook "webhook.serving.knative.dev": failed to call webhook: Post "https://webhook.knative-serving.svc:443/?timeout=10s": no endpoints available for service "webhook"
----

* When the `serving.knative.openshift.io/disableRoute=true` annotation is applied to a Knative Service, the service displays an invalid URL in the `.status.url` field. The URL shown does not resolve to the Knative Service and can be misleading. Additionally, both the OpenShift Console UI and the Knative client (`kn`) CLI display this invalid address in multiple locations. The corresponding Knative Route is also created, and its `.status.url` field contains the same invalid URL.

[id="known-issues-knative-cli-1-37-0_{context}"]
=== Knative client (`kn`) CLI

* As of {ServerlessProductName} 1.37 release, the `kn` client is built with RHEL 9 dependencies and cannot run on RHEL 8. Attempting to run the binary on RHEL 8 displays an error similar to the following:
+
[source,text]
----
kn: /lib64/libc.so.6: version `GLIBC_2.33' not found (required by kn)
----

[id="known-issues-osl-1-37-0_{context}"]
=== {ServerlessLogicProductName}

* In disconnected cluster environments, the `logic-swf-builder-rhel8` and `logic-swf-devmode-rhel8` images fail to download the Maven wrapper that is required to build and run workflow applications. This issue causes workflow builds to time out or fail during the Maven initialization phase.

* In disconnected cluster environments, the `logic-swf-builder-rhel8` image always attempts to download the `plexus-utils-1.1.jar` dependency during the build process. Because external network access is restricted in disconnected setups, this behavior can cause build failures or timeouts. 

* If you apply a SonataFlow custom resource (CR) to an OpenShift cluster and the first `SonataFlowBuild` fails for any reason, the Operator does not create the workflow deployment even after the build issue is resolved. As a result, the workflow remains undeployed until you manually re-apply or rebuild it.